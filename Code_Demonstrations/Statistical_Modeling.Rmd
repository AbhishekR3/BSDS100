---
title: "Model_Building_Examples"
author: "James D. Wilson"
date: "April 29, 2017"
output:
  pdf_document: default
  html_document: default
---

##Case Study I: Why are low-quality diamonds more expensive? Linear Models##
Here, we demonstrate an example of model building based on the `diamonds` data set in the `ggplot2` package. This code is borrowed from Chapter 24.2 in the *R for Data Science* book.

#Load all needed packages#
```{r, echo = TRUE, eval = TRUE}
install.packages(c("ggplot2", "tidyverse", "modelr", "hexbin"), repos = "https://cloud.r-project.org")
library(ggplot2, quiet = TRUE)
library(tidyverse, quiet = TRUE)
library(modelr, quiet = TRUE)
library(hexbin, quiet = TRUE)
```

#Exploratory Analysis of diamond quality and cost#

We want to investigate the relationship between diamond quality and cost so that we can a model that quantifies their relationship. To begin, we look at boxplots of the cost of diamonds of varying qualities. In particular, we investigate the relationship between `cost` and `cut`, `color` and `clarity` of the diamond.

```{r, echo = TRUE, eval = TRUE}
ggplot(diamonds, aes(cut, price)) + geom_boxplot()
ggplot(diamonds, aes(color, price)) + geom_boxplot()
ggplot(diamonds, aes(clarity, price)) + geom_boxplot()
```

It looks like lower quality diamonds have higher prices because there is an important confounding variable: the weight `carat` of the diamond. The weight of the diamond is the single most important factor for determining the price of the diamond, and lower quality diamonds tend to be larger. To investigate this further, we try a slightly different visualization.

```{r, echo = TRUE, eval = TRUE}
ggplot(diamonds, aes(carat, price)) + geom_hex(bins = 50)
```

We see two valuable trends in the above data. First, it appears that the `cost` is exponentially growing across the value of `carat`. This suggests that we could take a $log()$ transform of both variable to obtain a linear relationship between these variables. We do this and also focus our attention on the diamonds that are 2.5 carats or more in weight. We then plot our transformed variables.

```{r, echo = TRUE, eval = TRUE}
diamonds2 <- diamonds %>% 
  filter(carat <= 2.5) %>% 
  mutate(lprice = log2(price), lcarat = log2(carat))

ggplot(diamonds2, aes(lcarat, lprice)) + geom_hex(bins = 50)
```

We see that this transformation did a really good job at providing a **linear** relationship between these two variables! All we have to do now is fit a linear model. This is readily done with the `lm()` function as follows:

**Fitting a Linear Model**

The *lm()* function
-------------------------------

Let's first look at the help file for the `lm()` function. This function is the function to use if you are interested in fitting a **linear model**.

```{r, echo = TRUE}
?lm
```

**Important Arguments**

- *x*: input design matrix
- *y*: response variable


We wish to model the response `lprice` against the predictor `lcarat` in the following linear model:

$$\log(price) = \beta_0 + \beta_1\log(carat) + \epsilon$$
where above $\beta_0$ and $\beta_1$ are unknown **coefficient** values. The aim of the `lm` function is to estimate or fit these coefficient values. We see that next.

```{r, echo = TRUE, eval = TRUE}
mod_diamond <- lm(lprice ~ lcarat, data = diamonds2)
```

`mod_diamond` above is a `lm` object, and contains a lot of information that can be used for inference, such as coefficient values, p-values that can be used to assess whether or not coefficients are statistically significant. To look at the coefficients, we can try a few of the following functions:

```{r, echo = TRUE, eval = TRUE}
#get a summary of the fit
summary(mod_diamond)

#get the coefficients of the fit
coef(mod_diamond)
```
So, this is saying that the model we fit is given by the following linear regression:

$$\log(price) = 12.1939 + 1.6813\log(carat)$$
Also, from the `summary` results, we see that the p-values for both coefficients (represented by `Pr(>|t|))`) are less than 2e-16, suggesting that each coefficient is statistically significant.

Secondly, we see that we get an Adjusted $R^2$ of 0.9334, meaning that the variable `carat` explains 93% of the variability of the `price`.

#Visualize the fitted model to the data
```{r, echo = TRUE, eval = TRUE}
#make predictions on for each carat value within the values that we observed (this is interpolation!) Call these values grid.
grid <- diamonds2 %>% 
  data_grid(carat = seq_range(carat, 20)) %>% 
  mutate(lcarat = log2(carat)) %>% 
  add_predictions(mod_diamond, "lprice") %>% 
  mutate(price = 2 ^ lprice)

#Plot the predictions (grid) on the original scale of the carat and price
ggplot(diamonds2, aes(carat, price)) + 
  geom_hex(bins = 50) + 
  geom_line(data = grid, colour = "red", size = 1)

```

Importantly, we see that our fitted model was a linear model on `log(price)` against `log(carat)`. We interpolate values of `grid` by fitting our fitted model on the original `price` and `carat` scale, which it turns out, is an exponential model. 

#Residual Analysis
Once you've fit a model, you obtain predicted values $\widehat{y_i}$ for each response observation $y_i$. An important value to investigate is the **residual**, given by $\widehat{y_i} - y_i$. 

There are a lot of things we could look into here, but essentially, a good rule of thumb is: *your model is a good fit if your residuals are randomly scattered about 0*. That is, if you see any trend in the residuals, you may need to include another predictor to capture this variability.

To obtain and plot the residuals of this fitted model, we can use the following script.

```{r, echo = TRUE, eval = TRUE}
diamonds2 <- diamonds2 %>% 
  add_residuals(mod_diamond, "lresid")

ggplot(diamonds2, aes(lcarat, lresid)) + 
  geom_hex(bins = 50)

```

This plot shows that there are some diamonds that have quite large residuals. It is advisable to dive more into the data more to manually investigate these diamonds. But we don't do it here for the sake of time. Other than that observation, the residuals look fairly randomly scattered about 0.

##Models Part II: Shrinkage Methods
we will explore how to use three shrinkage methods:

1) Ridge Regression
2) The Lasso
3) Elastic Net

We will be utilizing an important R package *glmnet*, which was constructed by Hastie and Tibshirani to implement these three methods in a convenient way. Much of this lab is a replication of Section 6.6 in the "Introduction to Statistical Learning" textbook by James, Witten, Hastie, and Tibshirani (with my own commentary throughout).

**Install Needed Packages**
```{r, echo = TRUE}
install.packages("glmnet", repos='http://cran.us.r-project.org')
install.packages("ISLR", repos='http://cran.us.r-project.org')
library(glmnet, quietly = TRUE)
library(ISLR, quietly = TRUE)
```

The *glmnet()* function
-------------------------------

Let's first look at the help file for the *glmnet()* function. This function is a workhorse for all three methods that we are interested in implementing. We'll come back to this over and over again for each method.

```{r, echo = TRUE}
?glmnet
```

**Important Arguments**

- *x*: input design matrix
- *y*: response variable
- *family*: the family of regressions we want to use. For linear regression, we use "gaussian"; however, later we will look at other families such as "binomial" for logistic regression.
- *lambda*: the grid of possible $\lambda$ values for regularization
- *alpha*: this function is created to naturally handle elastic net. The penalties are given by: 
  $$\frac{1-\alpha}{2} \lambda \ell_2^2(\beta) + \alpha \lambda \ell_1(\beta)$$ 
So, $\alpha = 1$ gives Lasso, and $\alpha = 0$ gives Ridge regression.
- *standardize*: logical indicating whether or not the covariates should be standardized to be on the same scale. This is important for interpretation of coefficients. The default is TRUE. 


The Data
-------------------------------
The data set that we will investigate is the *Hitters* data in the *ISLR* package. This data contains Major League Baseball statistics for players during the 1986 and 1987 seasons. Let's look a bit at the data

```{r, echo = TRUE}
attach(Hitters)
?Hitters
names(Hitters)
```

Now we'll check if there is any missing data. 
```{r, echo = TRUE}
sum(is.na(Hitters))
```

Indeed, there are 59 missing entries in the *Hitters* data set. We could try to impute these data, but for simplicity, we choose to just ignore them and remove all entries from the data set.

```{r, echo = TRUE}
Hitters <- na.omit(Hitters)
```

Our goal is to understand the relationship between a player's **Salary** and a player's other MLB characteristics throughout the 1986-1987 seasons. Let's prepare the response and design matrices for later analysis.

```{r, echo = TRUE}
x <- model.matrix(Salary ~., data = Hitters)[, -1]
y <- Hitters$Salary
```

The *model.matrix()* function above automatically sets the classes of each covariate appropriately changing categorical data to *factors*. This will make the use of *glmnet()* much easier.


Ridge Regression
--------------------------------------------

Now, we will run ridge regression of the *Salary* data against the remaining predictors in the *Hitters* data set. Note that first we specify a grid of $\lambda$ values to search over. *glmnet()* will fit a model for each value of $\lambda$.

```{r, echo = TRUE}
#First, set a grid of lambda to search over. We want to include lambda = 0 for standard linear regression
grid.lambda <- 10^seq(10, -2, length = 100)

#Fit the model across the grid of lambda values
ridge.model <- glmnet(x, y, alpha = 0, lambda = grid.lambda)

#Plot the L1 norm of the coefficents
plot(ridge.model)

```

Associated with each value of $\lambda$, we have coefficient estimates that can be viewed using the *coef()* function. As there are 100 values of $\lambda$ and 20 coefficients, the coefficents are given in a $20 \times 100$ table. We look at an example of the coefficients and then calculate their $\ell_2$ norm. We then do this across all lambda and plot the $\ell_2$ norm at each value.

```{r, echo = TRUE}
#Look at the 50th value of lambda
ridge.model$lambda[50]

#Coefficients for this value 
coef(ridge.model)[, 50]

#Norm of these variables
sqrt(sum(coef(ridge.model)[-1, 50]^2))

#Let's repeat this for all lambda values and plot the results
ell2.norm <- numeric()
for(i in 1:length(grid.lambda)){
  ell2.norm[i] <- sqrt(sum(coef(ridge.model)[-1, i]^2))
}

plot(x = grid.lambda, y = ell2.norm, xlab = expression(lambda), ylab = "L2 Norm", xlim = c(10,10000))
```

**Cross-Validation and Choosing the *best* tuning parameter**

Using cross validation, we will now choose an optimal tuning parameter. To do so, we use the *cv.glmnet()*. By default, the function performs ten-fold cross-validation; however, we can change this by changing the argument *nfolds*. Let's see how this works.

```{r, echo = TRUE}
set.seed(1) #for reproducability
#Randomly select a training and test set.
#Here, we leave half of the data out for later model assessment
train <- sample(1:nrow(x), nrow(x) / 2)
test <- (-train)
y.train <- y[train]
y.test <- y[test]

#Now, fit a Ridge regression model to the training data
ridge.model.train <- glmnet(x[train, ], y.train, alpha = 0, lambda = grid.lambda)

#Let's perform cross validation to choose the best model
?cv.glmnet

#Perform cross validation on the training set to select the best lambda
set.seed(1) #for reproducability
cv.out <- cv.glmnet(x[train, ], y.train, alpha = 0)
plot(cv.out)

#Find the best lambda value
best.lambda <- cv.out$lambda.min
best.lambda
plot(cv.out)
abline(v = log(best.lambda), col = "blue", lwd = 2)

#Calculate the MSPE of the model on the test set
ridge.pred <- predict(ridge.model.train, s = best.lambda, newx = x[test, ])
mspe.ridge <- mean((ridge.pred - y.test)^2)
mspe.ridge

#Fit the final model to the entire data set using the chosen lambda
final.model <- glmnet(x, y, alpha = 0, lambda = best.lambda)
Coef.Ridge <- coef(final.model)[1:20, ]
Coef.Ridge
```

**Important Notes**:

1) *glmnet()* does *not* give standard errors for estimates! To do this, one would need to re-fit using a linear regression or directly calculate these values using formula given in class.

2) As expected, Ridge regression does *not* shrink any variables exactly to 0! For this, we need to use something like the Lasso.

Lasso
-------------------------------

Thankfully, *glmnet()* is readily available for Lasso and only requires the change of a single argument: *alpha*. For Lasso, we need to set *alpha* to 1 instead of 0 as done in the case for Ridge regression. Below, I put all of the code for running Lasso for demonstration. In reality, this is simply a copy-paste of the above code now changing *alpha* as needed. At the end of this section, we will discuss the similarities and differences between Lasso and Ridge regression.

**Shrinkage Effects on the Entire Model**

```{r, echo = TRUE}
#First, let's look at the shrinkage effects of Lasso on the entire data set
lasso.model <- glmnet(x, y, alpha = 1, lambda = grid.lambda)
plot(lasso.model)
```

We already see that, unlike Ridge regression, there is a shrinkage effect over an interval of lambda values. This agrees with our theoretical discussion in class. 

Now, let's fit the best model to the *Hitters* data and compare the results with Ridge regression. **Note**, we keep the training data and the test data the same so that we can have a fair comparison of methods.

```{r, echo = TRUE}

#Now, fit a Lasso regression model to the training data
lasso.model.train <- glmnet(x[train, ], y.train, alpha = 1, lambda = grid.lambda)


#Perform cross validation on the training set to select the best lambda
set.seed(1) #for reproducability
cv.out <- cv.glmnet(x[train, ], y.train, alpha = 1)
plot(cv.out)

#Find the best lambda value
best.lambda <- cv.out$lambda.min
best.lambda
plot(cv.out)
abline(v = log(best.lambda), col = "blue", lwd = 2)

#Calculate the MSPE of the model on the test set
lasso.pred <- predict(lasso.model.train, s = best.lambda, newx = x[test,])
mspe.lasso <- mean((lasso.pred - y.test)^2)
mspe.lasso

#Fit the final model to the entire data set using the chosen lambda
final.model <- glmnet(x, y, alpha = 1, lambda = best.lambda)
Coef.Lasso <- coef(final.model)[1:20,]
Coef.Lasso
```

Note that the final model here only includes 8 of the original 20 coefficients. Lasso did what we hoped! However, we see that the MSPE on the held-out test set (100743.4) was higher than that acheived by Ridge regression (96012.47). We see there is a tradeoff between prediction and interpretation of the model. 

Elastic Net
------------------------

Now, we repeat the same exercise above using a mixture of Lasso and Ridge regression methods. Here, we choose *alpha* = 0.50 (equal parts L1 and L2 penalties). Note again that the only thing changing from the above is the setting of alpha. 

**Shrinkage Effects on the Entire Model**

```{r, echo = TRUE}
#First, let's look at the shrinkage effects of Lasso on the entire data set
EN.model <- glmnet(x, y, alpha = 0.5, lambda = grid.lambda)
plot(EN.model)
```

There is some shrinkage that occurs with Elastic Net. In fact, it appears that the amount of shrinkage is between that of Ridge (no shrinkage) and the Lasso. 

Now, let's fit the best model to the *Hitters* data and compare the results with Ridge regression. **Note**, we keep the training data and the test data the same so that we can have a fair comparison of methods.

```{r, echo = TRUE}

#Now, fit a Lasso regression model to the training data
EN.model.train <- glmnet(x[train, ], y.train, alpha = 0.5, lambda = grid.lambda)


#Perform cross validation on the training set to select the best lambda
set.seed(1) #for reproducability
cv.out <- cv.glmnet(x[train, ], y.train, alpha = 0.5)
plot(cv.out)

#Find the best lambda value
best.lambda <- cv.out$lambda.min
best.lambda
plot(cv.out)
abline(v = log(best.lambda), col = "blue", lwd = 2)

#Calculate the MSPE of the model on the test set
EN.pred <- predict(EN.model.train, s = best.lambda, newx = x[test,])
mspe.EN <- mean((EN.pred - y.test)^2)
mspe.EN

#Fit the final model to the entire data set using the chosen lambda
final.model <- glmnet(x, y, alpha = 0.5, lambda = best.lambda)
Coef.EN <- coef(final.model)[1:20,]
Coef.EN
```
In the final model, we have 10 of the original 20 predictors.

**Comparison of the three models**
Let's compare the coefficients of each of these three models and the MSPE of each. 

```{r, echo = TRUE}
Coefficients <- data.frame(Ridge = Coef.Ridge, Lasso = Coef.Lasso, Elastic.Net = Coef.EN)
MSPE <- data.frame(Ridge = mspe.ridge, Lasso = mspe.lasso, Elastic.Net = mspe.EN)

Coefficients
MSPE
```

1) The coefficients kept by Lasso are a subset of those kept by Elastic Net.
2) Ridge regression keeps all coefficients (no shrinkage)
3) The MSPE is smaller for models with higher $p$. This shows a tradeoff between complexity and performance of the models.

**Key Point**: Linear regression and other statistical models will be covered much more thoroughly in the followup classes **MATH 372: Linear Regression** and **MATH 373: Statistical Learning** taught by Prof. Stevens and myself next Fall and Spring. So take these classes!